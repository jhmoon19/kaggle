{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8108f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('../../Tensorflow_NLP/chapter_7/bert-base-multilingual-cased')\n",
    "\n",
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent,\n",
    "        add_special_tokens = True,\n",
    "        max_length = MAX_LEN,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        truncation = True)\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "train_data = pd.read_csv('train.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('test.csv', encoding='latin-1')\n",
    "train_data = train_data.dropna(how='any', axis=1)\n",
    "\n",
    "train_data['text_len'] = train_data['text'].apply(lambda x:len(x.split(' ')))\n",
    "\n",
    "# 전처리\n",
    "\n",
    "# 1) url 제거 (https:///... www. ...)\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "# 2) 이모지, 이모티콘 제거\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# 3) html 태그 제거 (<a>, <br> ...)\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "\n",
    "# Special thanks to https://www.kaggle.com/tanulsingh077 for this function\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower() # 4) 소문자화, 문자열화\n",
    "    text = re.sub('\\[.*?\\]', '', text) # 대괄호 있으면 아예 다 제거 \n",
    "    \n",
    "    # remove_url()\n",
    "    text = re.sub(\n",
    "        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
    "        '', \n",
    "        text\n",
    "    )\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    \n",
    "    # 5) 구두점 제거 \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    \n",
    "    # 6) 숫자, 숫자포함단어는 모두 제거\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    text = remove_url(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_html(text)\n",
    "    \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 7) 불용어 제거\n",
    "stop_words = stopwords.words('english') # 179가지 nltk 영어 불용어\n",
    "more_stopwords = ['u', 'im', 'c'] # 179 + 3(축약,변환형)\n",
    "stop_words = stop_words + more_stopwords # 182개 불용어\n",
    "\n",
    "# # nltk의 SnowballStemmer('english') --> stem()\n",
    "# # nltk.PorterStemmer, nltk.LancasterStemmer 존재 \n",
    "# # Stemmer: 접사 삭제, 어간부 추출 / Lemmatizer: 어간 표제형 복원 (시간 오래걸림)\n",
    "# # nltk.WordNetLemmatizer() --> lemma()\n",
    "# stemmer = nltk.SnowballStemmer(\"english\")\n",
    "# # 8) 어간 Stemming\n",
    "\n",
    "def preprocess_data(text):\n",
    "    # Clean puntuation, urls, and so on\n",
    "    text = clean_text(text)\n",
    "    # Remove stopwords and Stemm all the words in the sentence\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "test_data['text_clean'] = test_data['text'].apply(preprocess_data)\n",
    "train_data['text_clean'] = train_data['text'].apply(preprocess_data)\n",
    "\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a60ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7613/7613 [00:01<00:00, 4727.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101  37246  10107  27949  63406  11387  10512  10237  10142 106088\n",
      "  19626    102      0      0      0      0      0      0      0      0\n",
      "      0      0      0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[CLS] deeds reason earthquake may allah forgive us [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in \\\n",
    "tqdm(zip(train_data['text_clean'], train_data['target']),\n",
    "    total = len(train_data)):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = \\\n",
    "        bert_tokenizer(train_sent, 23)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        train_data_labels.append(train_label)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n",
    "    \n",
    "train_tweet_input_ids = np.array(input_ids, dtype=int)\n",
    "train_tweet_attention_masks = np.array(attention_masks, dtype=int)\n",
    "train_tweet_type_ids = np.array(token_type_ids, dtype=int)\n",
    "train_tweet_inputs = (train_tweet_input_ids, train_tweet_attention_masks, train_tweet_type_ids)\n",
    "\n",
    "train_data_labels = np.array(train_data_labels, dtype=np.int32)\n",
    "\n",
    "input_id, attention_mask, token_type_id = [i[0] for i in train_tweet_inputs]\n",
    "print(input_id, attention_mask, token_type_id, tokenizer.decode(input_id), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7412d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.78654932352555"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([len(tokenizer.encode(j)) for j in train_data['text_clean']])\n",
    "\n",
    "arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97b7aa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.78654932352555"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8338c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd460516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(arr, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c3586ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../Tensorflow_NLP/chapter_7/bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at ../../Tensorflow_NLP/chapter_7/bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = Dense(num_class, \n",
    "                               kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                               name = 'classifier')\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        # Dropout(training=False) 파라미터의 의미?\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "cls_model = TFBertClassifier(model_name = \"../../Tensorflow_NLP/chapter_7/bert-base-multilingual-cased\",\n",
    "                            dir_path = 'bert_ckpt',\n",
    "                            # 이 경로에 해당 모델 cache 저장됨\n",
    "                            num_class = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96415dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "optimizer = Adam(3e-5)\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "# 그냥 'binary_crossentropy' 와의 차이점? \n",
    "metric = SparseCategoricalAccuracy('accuracy')\n",
    "# 그냥 \"accuracy\" 와의 차이점? \n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c54cd54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf2_bert_disaster_tweets_v4 -- Folder create complete \n",
      "\n",
      "Epoch 1/15\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.7663\n",
      "Epoch 1: val_accuracy improved from -inf to 0.81550, saving model to tf2_bert_disaster_tweets_v4\\weights.h5\n",
      "191/191 [==============================] - 1107s 6s/step - loss: 0.5063 - accuracy: 0.7663 - val_loss: 0.4179 - val_accuracy: 0.8155\n",
      "Epoch 2/15\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4031 - accuracy: 0.8287\n",
      "Epoch 2: val_accuracy did not improve from 0.81550\n",
      "191/191 [==============================] - 1241s 7s/step - loss: 0.4031 - accuracy: 0.8287 - val_loss: 0.4445 - val_accuracy: 0.8030\n",
      "Epoch 3/15\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8821\n",
      "Epoch 3: val_accuracy did not improve from 0.81550\n",
      "191/191 [==============================] - 1200s 6s/step - loss: 0.3101 - accuracy: 0.8821 - val_loss: 0.5161 - val_accuracy: 0.7951\n",
      "Epoch 4/15\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9094\n",
      "Epoch 4: val_accuracy did not improve from 0.81550\n",
      "191/191 [==============================] - 1712s 9s/step - loss: 0.2533 - accuracy: 0.9094 - val_loss: 0.5484 - val_accuracy: 0.7945\n",
      "{'loss': [0.5062862038612366, 0.4031009078025818, 0.31013819575309753, 0.25328850746154785], 'accuracy': [0.7663382887840271, 0.8287356495857239, 0.882101833820343, 0.9093596339225769], 'val_loss': [0.4178755283355713, 0.44452497363090515, 0.51612389087677, 0.5483962297439575], 'val_accuracy': [0.8154957294464111, 0.8030203580856323, 0.7951411604881287, 0.794484555721283]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_disaster_tweets_v4\"\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy',\n",
    "                                  min_delta=0.0001, patience=3)\n",
    "\n",
    "checkpoint_path = os.path.join(model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy',\n",
    "                             verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = cls_model.fit(train_tweet_inputs, train_data_labels,\n",
    "                       epochs=15, batch_size=32,\n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [earlystop_callback, cp_callback])\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27d471f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3263/3263 [00:00<00:00, 3832.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sents: 7613\n",
      "102/102 [==============================] - 98s 933ms/step\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "# train_data_labels = []\n",
    "\n",
    "for test_sent in tqdm(test_data['text_clean'], total = len(test_data)):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id =\\\n",
    "        bert_tokenizer(test_sent, 23)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "#         train_data_labels.append(train_label)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n",
    "    \n",
    "test_tweet_input_ids = np.array(input_ids, dtype=int)\n",
    "test_tweet_attention_masks = np.array(attention_masks, dtype=int)\n",
    "test_tweet_type_ids = np.array(token_type_ids, dtype=int)\n",
    "test_tweet_inputs = (test_tweet_input_ids, test_tweet_attention_masks, test_tweet_type_ids)\n",
    "\n",
    "# test_data_labels = np.array(train_data_labels, dtype=np.int32)\n",
    "\n",
    "print(\"# sents: {}\".format(len(train_tweet_input_ids)))\n",
    "\n",
    "y_pre = cls_model.predict(test_tweet_inputs)\n",
    "sub = pd.DataFrame({'id':test_data['id'].values.tolist(), \n",
    "                    'target': [i.argmax() for i in y_pre]})\n",
    "sub.to_csv(\"submission_5_bert_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7609a66",
   "metadata": {},
   "source": [
    "  0.79272 --> SnowballStemmer 스테밍 한 버트 버전보다 좀더 높음 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
