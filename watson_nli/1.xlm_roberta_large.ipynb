{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e1279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 번역 패키지\n",
    "!pip install -q googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37b1510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n",
      "Transformers version: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "import gc \n",
    "import os\n",
    "import random\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from googletrans import Translator\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "# 노트북 상 버전은 2.2.0,, 3.0.2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b52578",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "- configuration class is setup to define as many levers required for experiments as possible. Following things are changable by changing only one line of code.\n",
    "    - different huggingface models with Tensorflow\n",
    "    - different hyperparameter spaces for models\n",
    "    - different seeds, splits, accelerators\n",
    "    - different learning rate schedulers (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca787f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        translation=True,\n",
    "        max_length=64,\n",
    "        padding=True,\n",
    "        batch_size=128,\n",
    "        epochs=5,\n",
    "        learning_rate=1e-5,\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "        verbose=1,\n",
    "        train_splits=5,\n",
    "        accelerator=\"TPU\",\n",
    "        myluckynumber=13\n",
    "    ):\n",
    "        self.SEED = myluckynumber\n",
    "        self.ACCELERATOR = accelerator\n",
    "        \n",
    "        # from pathlib import Path\n",
    "        self.PATH_TRAIN = Path(\"./train.csv\")\n",
    "        self.PATH_TEST = Path(\"./test.csv\")\n",
    "        \n",
    "        self.TRAIN_SPLITS = train_splits\n",
    "        \n",
    "        self.LANGUAGE_MAP = {\n",
    "            \"English\":0,\n",
    "            \"Chinese\":1,\n",
    "            \"Arabic\":2,\n",
    "            \"French\":3,\n",
    "            \"Swahili\":4,\n",
    "            \"Urdu\":5,\n",
    "            \"Vietnamese\":6,\n",
    "            \"Russian\":7,\n",
    "            \"Hindi\":8,\n",
    "            \"Greek\":9,\n",
    "            \"Thai\":10,\n",
    "            \"Spanish\":11,\n",
    "            \"German\":12,\n",
    "            \"Turkish\":13,\n",
    "            \"Bulgarian\":14\n",
    "        }\n",
    "        \n",
    "        self.INVERSE_LANGUAGE_MAP = {v:k for k,v in self.LANGUAGE_MAP.items()}\n",
    "        \n",
    "        # model configuration\n",
    "        self.MODEL_NAME = model_name\n",
    "        self.TRANSLATION = translation\n",
    "        # from transformers import AutoTokenizer\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_NAME)\n",
    "        \n",
    "        # model hyperparameters\n",
    "        self.MAX_LENGTH = max_length\n",
    "        self.PAD_TO_MAX_LENGTH = padding\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.EPOCHS = epochs\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "        self.METRICS = metrics\n",
    "        self.VERBOSE = verbose\n",
    "        \n",
    "        # initializing accelerator\n",
    "        self.initialize_accelerator()\n",
    "        \n",
    "    def initialize_accelerator(self):\n",
    "        \n",
    "        # checking TPU first\n",
    "        if self.ACCELERATOR == \"TPU\":\n",
    "            print(\"Connecting to TPU\")\n",
    "            try:\n",
    "                tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "                print(f\"Running on TPU {tpu.master()}\")\n",
    "            except ValueError:\n",
    "                print(\"Could not connect to TPU\")\n",
    "                tpu = None\n",
    "                \n",
    "            if tpu:\n",
    "                try:\n",
    "                    print(\"Initializing TPU\")\n",
    "                    tf.config.experimental_connect_to_cluster(tpu)\n",
    "                    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "                    self.strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "                    self.tpu = tpu\n",
    "                    print(\"TPU initialized\")\n",
    "                except _:\n",
    "                    print(\"Failed to initialize TPU\")\n",
    "            else:\n",
    "                print(\"Unable to initialize TPU\")\n",
    "                self.ACCELERATOR = \"GPU\"\n",
    "        \n",
    "        # default for CPU and GPU\n",
    "        if self.ACCELERATOR != 'TPU':\n",
    "            print(\"Using default strategy for CPU and single GPU\")\n",
    "            self.strategy = tf.distribute.get_strategy()\n",
    "            \n",
    "        # checking GPUs\n",
    "        if self.ACCELERATOR == \"GPU\":\n",
    "            print(f\"GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "            \n",
    "        # defining replicas(복제품)\n",
    "        self.AUTO = tf.data.experimental.AUTOTUNE\n",
    "        self.REPLICAS = self.strategy.num_replicas_in_sync\n",
    "        print(f\"REPLICAS: {self.REPLICAS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8cdec2",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- Google Translator 이용 전부 영어로 바꾸기 \n",
    "- tf.data.Dataset으로 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a58fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text_to_english(text):\n",
    "    translator = Translator()\n",
    "    return translator.translate(text, dest='en').text\n",
    "\n",
    "def encode_text(df, tokenizer, max_len, padding):\n",
    "    text = df[['premise','hypothesis']].values.tolist()\n",
    "    \n",
    "    text_encoded = tokenizer.batch_encode_plus(\n",
    "        text,\n",
    "        pad_to_max_length = padding,\n",
    "        max_length = max_len)\n",
    "    \n",
    "    return text_encoded\n",
    "\n",
    "def get_tf_dataset(x,y,auto,labeled=True,repeat=False,shuffle=False,batch_size=128):\n",
    "    \"\"\" creating tf.data.Dataset for TPU \"\"\"\n",
    "    \n",
    "    if labeled:\n",
    "        ds = (tf.data.Dataset.from_tensor_slices((x['input_ids'],y)))\n",
    "    else:\n",
    "        ds = (tf.data.Dataset.from_tensor_slices(x['input_ids']))\n",
    "        \n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "        \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(2048)\n",
    "        \n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(auto)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c55ce",
   "metadata": {},
   "source": [
    "# Deep Learning model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12385686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name, max_len, learning_rate, metrics):\n",
    "    input_ids = Input(shape=(max_len,),dtype=tf.int32,name='input_ids')\n",
    "    \n",
    "    transformer_model = TFAutoModel.from_pretrained(model_name)\n",
    "    transformer_embeddings = transformer_model(input_ids)[0]\n",
    "    \n",
    "    output_values = Dense(3, activation='softmax')(transformer_embeddings[:,0,:])\n",
    "    \n",
    "    # 모델 정의\n",
    "    model = Model(inputs=input_ids, outputs=output_values)\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics=metrics\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734a35b",
   "metadata": {},
   "source": [
    "# stratified k-fold modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca6a2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(config):\n",
    "    df_train = pd.read_csv(config.PATH_TRAIN)\n",
    "    df_test = pd.read_csv(config.PATH_TEST)\n",
    "    \n",
    "    if config.TRANSLATION:\n",
    "        df_train.loc[df_train.language != 'English','premise'] =\\\n",
    "        df_train[df_train.language != 'English'].premise.apply\\\n",
    "        (lambda x:translate_text_to_english(x))\n",
    "        \n",
    "        df_test.loc[df_test.language != 'English','premise'] =\\\n",
    "        df_test[df_test.language != 'English'].premise.apply\\\n",
    "        (lambda x:translate_text_to_english(x))\n",
    "        \n",
    "        df_train.loc[df_train.language != 'English','hypothesis'] =\\\n",
    "        df_train[df_train.language != 'English'].hypothesis.apply\\\n",
    "        (lambda x:translate_text_to_english(x))\n",
    "        \n",
    "        df_test.loc[df_test.language != 'English','hypothesis'] =\\\n",
    "        df_test[df_test.language != 'English'].hypothesis.apply\\\n",
    "        (lambda x:translate_text_to_english(x))\n",
    "        \n",
    "    # adding column for stratified splitting\n",
    "    df_train['language_label'] = df_train.language.astype(str)+'_'+df_train.label.astype(str)\n",
    "\n",
    "    # stratified k-fold on language and label\n",
    "    skf = StratifiedKFold(n_splits=config.TRAIN_SPLITS, shuffle=True, random_state=config.SEED)\n",
    "\n",
    "    # initializing predictions\n",
    "    preds_oof = np.zeros((df_train.shape[0], 3))\n",
    "    preds_test = np.zeros((df_test.shape[0], 3))        \n",
    "    acc_oof = []\n",
    "\n",
    "    # iterating over folds\n",
    "    for (fold, (train_index,valid_index)) in enumerate(skf.split(df_train,df_train.language_label)):\n",
    "\n",
    "        # initializing TPU\n",
    "        if config.ACCELERATOR == 'TPU':\n",
    "            if config.tpu:\n",
    "                config.initialize_acceletor()\n",
    "\n",
    "        # building model\n",
    "        # from tensorflow.keras.backend import K\n",
    "        K.clear_session()\n",
    "        with config.strategy.scope():\n",
    "            model = build_model(config.MODEL_NAME, \n",
    "                                config.MAX_LENGTH, \n",
    "                                config.LEARNING_RATE, \n",
    "                                config.METRICS)\n",
    "            if fold == 0:\n",
    "                print(model.summary())\n",
    "\n",
    "        print('\\n')\n",
    "        print('#'*19)\n",
    "        print(f\"##### Fold: {fold + 1} #####\")\n",
    "        print('#'*19)\n",
    "\n",
    "        x_train = df_train.iloc[train_index]\n",
    "        x_valid = df_train.iloc[valid_index]\n",
    "\n",
    "        y_train = x_train.label.values\n",
    "        y_valid = x_valid.label.values\n",
    "\n",
    "        print(\"\\nTokenizing\")\n",
    "\n",
    "        x_train_encoded = encode_text(df=x_train, \n",
    "                                      tokenizer=config.TOKENIZER,\n",
    "                                      max_len=config.MAX_LENGTH,\n",
    "                                      padding=config.PAD_TO_MAX_LENGTH)\n",
    "\n",
    "        x_valid_encoded = encode_text(df=x_valid, \n",
    "                                      tokenizer=config.TOKENIZER,\n",
    "                                      max_len=config.MAX_LENGTH,\n",
    "                                      padding=config.PAD_TO_MAX_LENGTH)\n",
    "\n",
    "        ds_train = get_tf_dataset(x_train_encoded, y_train, config.AUTO,\n",
    "                                 repeat=True,shuffle=True,\n",
    "                                  batch_size=config.BATCH_SIZE*config.REPLICAS)\n",
    "        ds_valid = get_tf_dataset(x_valid_encoded, y_valid, config.AUTO,\n",
    "                                 batch_size=config.BATCH_SIZE*config.REPLICAS * 4)\n",
    "\n",
    "        n_train = x_train.shape[0]\n",
    "\n",
    "        if fold == 0:\n",
    "            x_test_encoded = encode_text(df=df_test,\n",
    "                                        tokenizer=config.TOKENIZER,\n",
    "                                        max_len=config.MAX_LENGTH,\n",
    "                                        padding=config.PAD_TO_MAX_LENGTH)\n",
    "\n",
    "            sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "                \"model.h5\",\n",
    "                monitor=\"val_sparse_categorical_accuracy\",\n",
    "                verbose=0,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                mode=\"max\",\n",
    "                save_freq=\"epoch\")\n",
    "\n",
    "            print(\"\\nTraining\")\n",
    "\n",
    "            model_history = model.fit(\n",
    "                ds_train,\n",
    "                epochs=config.EPOCHS,\n",
    "                callbacks=[sv],\n",
    "                steps_per_epoch=n_train/config.BATCH_SIZE//config.REPLICAS,\n",
    "                validation_data=ds_valid,\n",
    "                verbose=config.VERBOSE)\n",
    "\n",
    "            print(\"\\nValidating\")\n",
    "\n",
    "            # scoring validation data\n",
    "            model.load_weights(\"model.h5\")\n",
    "            ds_valid = get_tf_dataset(x_valid_encoded,-1,config.AUTO,\n",
    "                                     labeled=False,\n",
    "                                      batch_size=config.BATCH_SIZE*config.REPLICAS*4)\n",
    "\n",
    "            preds_valid = model.predict(ds_valid, verbose=config.VERBOSE)\n",
    "            acc = accuracy_score(y_valid, np.argmax(preds_valid, axis=1))\n",
    "\n",
    "            preds_oof[valid_index] = preds_valid\n",
    "            acc_oof.append(acc)\n",
    "\n",
    "            print(\"\\nInferencing\")\n",
    "\n",
    "            # scoring test data\n",
    "            ds_test = get_tf_dataset(x_test_encoded,-1,config.AUTO,\n",
    "                                    labeled=False,\n",
    "                                    batch_size=config.BATCH_SIZE*config.REPLICAS*4)\n",
    "\n",
    "            preds_test += model.predict(ds_test, verbose=config.VERBOSE)/config.TRAIN_SPLITS\n",
    "\n",
    "            print(f\"\\nFold {fold+1} Accuracy: {round(acc,4)}\\n\")\n",
    "\n",
    "            g = gc.collect()\n",
    "\n",
    "    # overall CV score and standard deviation\n",
    "    print(f\"\\nCV Mean Accuracy: {round(np.mean(acc_oof), 4)}\")\n",
    "    print(f\"CV StdDev Accuracy: {round(np.std(acc_oof), 4)}\\n\")\n",
    "\n",
    "    return preds_oof, preds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42156ec",
   "metadata": {},
   "source": [
    "# Experimenting with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc54516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to TPU\n",
      "Could not connect to TPU\n",
      "Unable to initialize TPU\n",
      "Using default strategy for CPU and single GPU\n",
      "GPUs Available: 0\n",
      "REPLICAS: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e070804c14457ca7396c5a5f1b33c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jplu/tf-xlm-roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at jplu/tf-xlm-roberta-large.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_ids (InputLayer)      [(None, 84)]              0         \n",
      "                                                                 \n",
      " tf_roberta_model (TFRoberta  ((None, 84, 1024),       559890432 \n",
      " Model)                       (None, 1024))                      \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 1024)             0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 3075      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 559,893,507\n",
      "Trainable params: 559,893,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "###################\n",
      "##### Fold: 1 #####\n",
      "###################\n",
      "\n",
      "Tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "Epoch 1/16\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  1/142 [..............................] - ETA: 13:51:15 - loss: 1.2114 - sparse_categorical_accuracy: 0.2656"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18548\\2440544924.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                         train_splits=4)\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpreds_train_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_test_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18548\\1772592230.py\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREPLICAS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 verbose=config.VERBOSE)\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nValidating\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m--> 135\u001b[1;33m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 53\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Final Model: XLM Roberta Large\n",
    "config_1 = Configuration(\"jplu/tf-xlm-roberta-large\",\n",
    "                        translation=False,\n",
    "                        max_length=84,\n",
    "                        batch_size=64,\n",
    "                        epochs=16,\n",
    "                        train_splits=4)\n",
    "preds_train_1, preds_test_1 = run_model(config_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3553bd",
   "metadata": {},
   "source": [
    "- 한 epoch당 13:51:15 걸림... ㅋㅋㅋ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
